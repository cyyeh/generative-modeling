{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigGAN nonlinear walk - Shift X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'notebooks/models/biggan_nonlinear_shiftx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick learning rate and number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "num_samples = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick step size and number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 25\n",
    "num_steps = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Graph and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory\n",
    "import os\n",
    "os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'output'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = 'https://tfhub.dev/deepmind/biggan-256/2'\n",
    "\n",
    "import io\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import time\n",
    "from resources import tf_lpips_pkg as lpips_tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "print('Loading BigGAN module from:', module_path)\n",
    "module = hub.Module(module_path)\n",
    "\n",
    "inputs = {k: tf.placeholder(v.dtype, v.get_shape().as_list(), k)\n",
    "          for k, v in module.get_input_info_dict().items()}\n",
    "output = module(inputs)\n",
    "\n",
    "print('Inputs:\\n', '\\n'.join(\n",
    "    '  {}: {}'.format(*kv) for kv in inputs.items()))\n",
    "print('Output:', output)\n",
    "\n",
    "input_z = inputs['z']\n",
    "input_y = inputs['y']\n",
    "input_trunc = inputs['truncation']\n",
    "dim_z = input_z.shape.as_list()[1]\n",
    "vocab_size = input_y.shape.as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input placeholders\n",
    "Nsliders = 1\n",
    "z = tf.placeholder(tf.float32, shape=(None, dim_z))\n",
    "y = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "truncation = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "# original output\n",
    "inputs_orig = {u'y': y,\n",
    "               u'z': z,\n",
    "               u'truncation': truncation}\n",
    "outputs_orig = module(inputs_orig)\n",
    "\n",
    "img_size = outputs_orig.shape[1].value\n",
    "num_channels = outputs_orig.shape[-1].value\n",
    "\n",
    "# output placeholders\n",
    "target = tf.placeholder(tf.float32, shape=(\n",
    "    None, img_size, img_size, num_channels))\n",
    "mask = tf.placeholder(tf.float32, shape=(\n",
    "    None, img_size, img_size, num_channels))\n",
    "\n",
    "# set walk parameters\n",
    "alpha = tf.placeholder(tf.int32, shape=())\n",
    "T1 = tf.keras.layers.Dense(dim_z, input_shape=(None, dim_z), kernel_initializer='RandomNormal', bias_initializer='zeros', activation=tf.nn.relu)\n",
    "T2 = tf.keras.layers.Dense(dim_z, input_shape=(None, dim_z), kernel_initializer='RandomNormal', bias_initializer='zeros')\n",
    "T3 = tf.keras.layers.Dense(dim_z, input_shape=(None, dim_z), kernel_initializer='RandomNormal', bias_initializer='zeros', activation=tf.nn.relu)\n",
    "T4 = tf.keras.layers.Dense(dim_z, input_shape=(None, dim_z), kernel_initializer='RandomNormal', bias_initializer='zeros')\n",
    "# forward transformation\n",
    "out_f = []\n",
    "z_prev = z\n",
    "z_norm = tf.norm(z, axis=1, keepdims=True)\n",
    "for i in range(1, num_steps + 1):\n",
    "    z_step = z_prev + T2(T1(z_prev))\n",
    "    z_step_norm = tf.norm(z_step, axis=1, keepdims=True)\n",
    "    z_step = z_step * z_norm / z_step_norm\n",
    "    out_f.append(z_step)\n",
    "    z_prev = z_step\n",
    "\n",
    "# reverse transformation\n",
    "out_g = []\n",
    "z_prev = z\n",
    "z_norm = tf.norm(z, axis=1, keepdims=True)\n",
    "for i in range(1, num_steps + 1):\n",
    "    z_step = z_prev + T4(T3(z_prev))\n",
    "    z_step_norm = tf.norm(z_step, axis=1, keepdims=True)\n",
    "    z_step = z_step * z_norm / z_step_norm\n",
    "    out_g.append(z_step)\n",
    "    z_prev = z_step\n",
    "out_g.reverse() # flip the reverse transformation\n",
    "\n",
    "# w has shape (2*num_steps + 1, batch_size, dim_z)\n",
    "# elements 0 to num_steps are the reverse transformation, in reverse order\n",
    "# elements num_steps + 1 to 2*num_steps + 1 are the forward transformation\n",
    "# element num_steps is no transformation\n",
    "w = tf.stack(out_g+[z]+out_f)\n",
    "\n",
    "# w is already z+f(z) so we can just index into w\n",
    "z_new = w[alpha,  :, :]\n",
    "\n",
    "transformed_inputs = {u'y': y,\n",
    "                      u'z': z_new,\n",
    "                      u'truncation': truncation}\n",
    "transformed_output = module(transformed_inputs)\n",
    "\n",
    "# losses\n",
    "loss = tf.losses.compute_weighted_loss(tf.square(\n",
    "    transformed_output-target), weights=mask)\n",
    "loss_lpips = tf.reduce_mean(lpips_tf.lpips(\n",
    "    mask*transformed_output, mask*target, model='net-lin', net='alex'))\n",
    "\n",
    "# train op \n",
    "# change to loss_lpips to loss to optimize l2 loss\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(\n",
    "    loss_lpips, var_list=tf.trainable_variables(scope=None), name='AdamOpter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto(log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(initializer)\n",
    "saver = tf.train.Saver(tf.trainable_variables(scope=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Target Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_np(outputs_zs, alpha, show_img=False, show_mask=False):\n",
    "    \n",
    "    mask_out = np.ones(outputs_zs.shape)\n",
    " \n",
    "    if alpha == 0:\n",
    "        return outputs_zs, mask_out\n",
    "    \n",
    "    M = np.float32([[1,0,alpha],[0,1,0]])\n",
    "    target_fn = np.zeros(outputs_zs.shape)\n",
    "    \n",
    "    for i in range(outputs_zs.shape[0]):\n",
    "        target_fn[i,:,:,:] = cv2.warpAffine(outputs_zs[i,:,:,:], M, (img_size, img_size))\n",
    "        mask_out[i,:,:,:] = cv2.warpAffine(mask_out[i,:,:,:], M, (img_size, img_size))\n",
    "\n",
    "    mask_out[np.nonzero(mask_out)] = 1.\n",
    "    assert(np.setdiff1d(mask_out, [0., 1.]).size == 0)\n",
    "\n",
    "    return target_fn, mask_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling operations\n",
    "from graphs.biggan.graph_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This can be train.py\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(output_dir, 'train')),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "logger = logging.getLogger()\n",
    "\n",
    "loss_vals = []\n",
    "\n",
    "\n",
    "# train\n",
    "def train(saver):\n",
    "    trunc=1.0\n",
    "    noise_seed=0\n",
    "    zs = truncated_z_sample(num_samples, trunc, noise_seed)\n",
    "    ys = np.random.randint(0,vocab_size,size=zs.shape[0])\n",
    "    ys = one_hot_if_needed(ys, vocab_size)\n",
    "\n",
    "    Loss_sum = 0\n",
    "    n_epoch = 1\n",
    "    Loss_sum_iter = 0\n",
    "    optim_iter = 0\n",
    "    batch_size = 4\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_start in range(0, num_samples, batch_size):\n",
    "            start_time = time.time()\n",
    "\n",
    "            alpha_val = np.random.randint(-num_steps, num_steps+1)\n",
    "            alpha_to_graph = alpha_val + num_steps\n",
    "            alpha_to_target = alpha_val * eps\n",
    "\n",
    "            s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "\n",
    "            feed_dict_out = {z: zs[s], y: ys[s], truncation: trunc}\n",
    "            out_zs = sess.run(outputs_orig, feed_dict_out)\n",
    "\n",
    "            target_fn, mask_out = get_target_np(out_zs, alpha_to_target)\n",
    "            \n",
    "            feed_dict = {z: zs[s], y: ys[s], truncation: trunc, alpha: alpha_to_graph, target: target_fn, mask: mask_out}\n",
    "            \n",
    "            curr_loss, _ = sess.run([loss, train_step], feed_dict=feed_dict)\n",
    "            Loss_sum = Loss_sum + curr_loss\n",
    "            Loss_sum_iter = Loss_sum_iter + curr_loss\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            logger.info('T, epc, bst, lss, a: {}, {}, {}, {}, {}'.format(elapsed_time, epoch, batch_start, curr_loss, alpha_to_target))\n",
    "\n",
    "\n",
    "            if (optim_iter % 100 == 0) and (optim_iter > 0):\n",
    "                saver.save(sess, './{}/{}/model_{}.ckpt'.format(output_dir, 'output', optim_iter*batch_size), write_meta_graph=False, write_state=False)\n",
    "\n",
    "            if (optim_iter % 100 == 0) and (optim_iter > 0):\n",
    "                loss_vals.append(Loss_sum_iter/(100*batch_size))\n",
    "                Loss_sum_iter = 0\n",
    "                print('Loss:', loss_vals)\n",
    "                \n",
    "            optim_iter = optim_iter+1\n",
    "            \n",
    "    if optim_iter > 0:\n",
    "        print('average loss with this metric: ', Loss_sum/(optim_iter*batch_size))\n",
    "    saver.save(sess, \"./{}/{}/model_{}.ckpt\".format(output_dir, 'output', optim_iter*batch_size), write_meta_graph=False, write_state=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image import imgrid, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To restore previous w:\n",
    "saver.restore(sess, \"./{}/{}/model_{}.ckpt\".format(output_dir, 'output', 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: show imgs \n",
    "# this can be test.py\n",
    "\n",
    "category = 207\n",
    "\n",
    "a = np.linspace(-200, 200, 9)\n",
    "\n",
    "trunc=0.5\n",
    "noise_seed=20   \n",
    "num_samples_vis = 6\n",
    "batch_size = 1\n",
    "\n",
    "zs = truncated_z_sample(num_samples_vis, trunc, noise_seed)\n",
    "ys = np.array([category] * zs.shape[0])\n",
    "ys = one_hot_if_needed(ys, vocab_size)\n",
    "\n",
    "for batch_num, batch_start in enumerate(range(0, num_samples_vis, batch_size)):\n",
    "\n",
    "    ims = []\n",
    "    targets = []\n",
    "\n",
    "    s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "\n",
    "    input_test = {y: ys[s],\n",
    "                  z: zs[s],\n",
    "                  truncation: trunc}\n",
    "\n",
    "    out_input_test = sess.run(outputs_orig, input_test)\n",
    "\n",
    "    for i in range(a.shape[0]):\n",
    "        target_fn, mask_out = get_target_np(out_input_test, a[i], show_img=False)\n",
    "        \n",
    "        \n",
    "        alpha_to_graph = int(np.round(a[i] / eps))\n",
    "        \n",
    "        direction = np.sign(alpha_to_graph)\n",
    "        steps = np.abs(alpha_to_graph)\n",
    "        single_step_alpha = num_steps + direction\n",
    "        # within the graph range, we can compute it directly\n",
    "        if 0 <= alpha_to_graph + num_steps <= num_steps * 2:\n",
    "            zs_out = sess.run(z_new, {\n",
    "                z:zs[s], alpha:\n",
    "                alpha_to_graph + num_steps})\n",
    "        else:\n",
    "            # recursive step\n",
    "            zs_next = zs[s]\n",
    "            for n in range(steps):\n",
    "                feed_dict = {z: zs_next, alpha: single_step_alpha}\n",
    "                zs_next = sess.run(z_new, feed_dict=feed_dict)\n",
    "            zs_out = zs_next\n",
    "        # already taken n steps at this point, so directly use zs_next\n",
    "        # without any further modifications: using num_steps index into w\n",
    "        # alternatively, could also use outputs_orig\n",
    "        best_inputs = {z: zs_out, y: ys[s],\n",
    "                       truncation: trunc, alpha: num_steps}\n",
    "        best_im_out = sess.run(transformed_output, best_inputs)\n",
    "        \n",
    "        # collect images\n",
    "        ims.append(np.uint8(np.clip(((best_im_out + 1) / 2.0) * 256, 0, 255)))\n",
    "        targets.append(np.uint8(np.clip(((target_fn + 1) / 2.0) * 256, 0, 255)))\n",
    "        \n",
    "    im_stack = np.concatenate(targets + ims).astype(np.uint8)\n",
    "    imshow(imgrid(im_stack, cols = len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_vals)\n",
    "plt.xlabel('num samples, lr{}'.format(lr))\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
