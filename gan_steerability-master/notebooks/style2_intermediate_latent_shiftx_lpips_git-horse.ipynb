{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# def get_available_gpus():\n",
    "#     local_device_protos = device_lib.list_local_devices()\n",
    "#     return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "# print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These two following cells are the only neccessary cells you need to modify compare to \n",
    "## StyleGAN linear walk - Color in W latent space notebook (refer to the same directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typeguard\n",
    "!git clone https://github.com/NVlabs/stylegan2.git\n",
    "%cd stylegan2\n",
    "!git clone https://github.com/kylemcdonald/python-utils.git utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "# import config\n",
    "import pretrained_networks\n",
    "\n",
    "from utils.imutil import imshow, imresize\n",
    "from utils.mosaic import make_mosaic\n",
    "\n",
    "import cv2  \n",
    "import time\n",
    "tf_lpips_pkg = __import__(\"lpips-tensorflow.lpips_tf\") \n",
    "\n",
    "tflib.init_tf()\n",
    "\n",
    "# Load pre-trained network.\n",
    "url = 'http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-horse-config-f.pkl' # 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
    "_G, _D, Gs = pretrained_networks.load_networks(url)\n",
    "img_size = 256 #horse: 256 #cat:256 #car:512  # face:1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsliders = 3\n",
    "dim_z = Gs.input_shape[1]\n",
    "\n",
    "z = tf.placeholder(tf.float32, shape=(None, dim_z))\n",
    "target = tf.placeholder(tf.float32, shape=(None, img_size, img_size, Nsliders))\n",
    "\n",
    "mask = tf.placeholder(tf.float32, shape=(None, img_size, img_size, Nsliders))\n",
    "\n",
    "alpha2 = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "w2 = tf.Variable(np.random.normal(0.0, 0.1, [1, 14, z.shape[1]]), name='walk_intermed', dtype=np.float32)\n",
    "\n",
    "outputs_orig = tf.transpose(Gs.get_output_for(z, None, is_validation=True, \n",
    "                                                    randomize_noise=True), [0, 2, 3, 1])\n",
    "\n",
    "out_dlatents = Gs.components.mapping.get_output_for(z, None)\n",
    "\n",
    "out_dlatents_new = out_dlatents+alpha2*w2\n",
    "    \n",
    "transformed_output = tf.transpose(Gs.components.synthesis.get_output_for(out_dlatents_new, is_validation=True, \n",
    "                                                    randomize_noise=True), [0, 2, 3, 1])\n",
    "\n",
    "## L_2 loss (if you want to try)\n",
    "loss = tf.losses.compute_weighted_loss(tf.square(transformed_output-target), weights=mask)\n",
    "\n",
    "# Lpips loss\n",
    "loss_lpips = tf.reduce_mean(tf_lpips_pkg.lpips_tf.lpips(mask*transformed_output, mask*target, \n",
    "                                                  model='net-lin', net='alex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars          = tf.global_variables()\n",
    "    is_not_initialized   = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    print([str(i.name) for i in not_initialized_vars]) # only for testing\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "        return not_initialized_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_sess = tf.get_default_session()\n",
    "not_initialized_vars = initialize_uninitialized(style_sess)\n",
    "\n",
    "lr = 1e-04\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss_lpips, var_list=not_initialized_vars, \n",
    "                                                 name='AdamOpter')\n",
    "# this time init Adam's vars:\n",
    "not_initialized_vars = initialize_uninitialized(style_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_np(outputs_zs, alpha, show_img=False, show_mask=False):\n",
    "    \n",
    "    mask_out = np.ones(outputs_zs.shape)\n",
    " \n",
    "    if alpha == 0:\n",
    "        return outputs_zs, mask_out\n",
    "    \n",
    "    M = np.float32([[1,0,alpha],[0,1,0]])\n",
    "    target_fn = np.zeros(outputs_zs.shape)\n",
    "    \n",
    "    for i in range(outputs_zs.shape[0]):\n",
    "        target_fn[i,:,:,:] = cv2.warpAffine(outputs_zs[i,:,:,:], M, (img_size, img_size))\n",
    "        mask_out[i,:,:,:] = cv2.warpAffine(mask_out[i,:,:,:], M, (img_size, img_size))\n",
    "\n",
    "    mask_out[np.nonzero(mask_out)] = 1.\n",
    "    assert(np.setdiff1d(mask_out, [0., 1.]).size == 0)\n",
    "        \n",
    "    if show_img:\n",
    "        img_show_size = 128\n",
    "        print('Target image:')\n",
    "        # just for showing, i need ot scale it so for NOW i am using the original scaling in tflib.convert_images_to_uint8\n",
    "        im = style_sess.run(tflib.convert_images_to_uint8(tf.convert_to_tensor(target_fn, dtype=tf.float32)))\n",
    "        \n",
    "        for b in range(outputs_zs.shape[0]):\n",
    "            images_resized = np.array(PIL.Image.fromarray(im[b,:,:,:]).resize((img_show_size, img_show_size)))\n",
    "            imshow(images_resized)\n",
    "            \n",
    "    if show_mask:\n",
    "        img_show_size = 128\n",
    "        print('Target mask:')\n",
    "        for b in range(outputs_zs.shape[0]):\n",
    "            mask_tmp = (mask_out[b,:,:,:]*255).astype(np.uint8)\n",
    "            images_resized = np.array(PIL.Image.fromarray(mask_tmp).resize((img_show_size, img_show_size)))\n",
    "            imshow(images_resized)\n",
    "\n",
    "    return target_fn, mask_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = '../shiftx_intermed_lpips_git_linear_horse_40k_{}'.format(lr)\n",
    "os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'output'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.trainable_variables(scope=None))\n",
    "# saver.restore(tf.get_default_session(), \"./shiftx_intermed_lpips_git_linear_horse_40k_0.0001/model_5000_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be train.py\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}/{1}.log\".format(output_dir, 'train')),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "alpha_list = []\n",
    "loss_vals = []\n",
    "\n",
    "# train\n",
    "def train(saver):\n",
    "    num_samples=40000\n",
    "    random_seed = 1\n",
    "    rnd = np.random.RandomState(random_seed)\n",
    "    zs = rnd.randn(num_samples, dim_z)\n",
    "\n",
    "    Loss_sum = 0\n",
    "    Loss_sum_iter = 0\n",
    "    n_epoch = 1\n",
    "    optim_iter = 0\n",
    "    batch_size = 4\n",
    "    alpha_max = 100\n",
    "    alpha_range = np.arange(5, alpha_max+5, 5)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_start in range(0, num_samples, batch_size):\n",
    "            start_time = time.time()\n",
    "\n",
    "            alpha_val = np.random.choice(alpha_range)\n",
    "            ## lets flip a coin\n",
    "            coin = np.random.uniform(0, 1)\n",
    "            if coin <= 0.5:\n",
    "                alpha_val = -alpha_val\n",
    "                \n",
    "            s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "\n",
    "            feed_dict_out = {z: zs[s]}\n",
    "            out_zs = style_sess.run(outputs_orig, feed_dict_out) \n",
    "            target_fn, mask_out = get_target_np(out_zs, alpha_val)#, show_img=True, show_mask=True)\n",
    "            \n",
    "            feed_dict = {z: zs[s], alpha2: alpha_val/alpha_max, target: target_fn, mask: mask_out}\n",
    "            \n",
    "            curr_loss, _  = style_sess.run([loss_lpips, train_step], feed_dict=feed_dict)\n",
    "            Loss_sum = Loss_sum + curr_loss\n",
    "            Loss_sum_iter = Loss_sum_iter + curr_loss\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            logger.info('T, epc, bst, lss, a: {}, {}, {}, {}, {}'.format(elapsed_time, epoch, batch_start, curr_loss, alpha_val))\n",
    "\n",
    "            alpha_list.append(alpha_val)\n",
    "\n",
    "            if (optim_iter % 2500 == 0) and (optim_iter > 0):\n",
    "                saver.save(style_sess, '{}/{}/model_{}.ckpt'.format(output_dir, 'output', optim_iter*batch_size), write_meta_graph=False, write_state=False)\n",
    "            \n",
    "            if (optim_iter % 100 == 0) and (optim_iter > 0):\n",
    "                loss_vals.append(Loss_sum_iter/(100*batch_size))\n",
    "                Loss_sum_iter = 0\n",
    "                print('Loss:', loss_vals)\n",
    "\n",
    "            optim_iter = optim_iter+1\n",
    "            \n",
    "    if optim_iter > 0:\n",
    "        loss_vals.append(Loss_sum_iter/(100*batch_size))\n",
    "        print('average loss with this metric: ', Loss_sum/(optim_iter*batch_size))\n",
    "    saver.save(style_sess, '{}/{}/model_{}.ckpt'.format(output_dir, 'output', optim_iter*batch_size), write_meta_graph=False, write_state=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the stylegan output range ([-1, 1]) to uint8 range [0, 255]\n",
    "float_im = tf.placeholder(tf.float32, outputs_orig.shape)\n",
    "uint8_im = tflib.convert_images_to_uint8(tf.convert_to_tensor(float_im, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some showing utils\n",
    "import io\n",
    "import IPython.display\n",
    "\n",
    "def imgrid(imarray, cols=5, pad=1):\n",
    "    if imarray.dtype != np.uint8:\n",
    "        raise ValueError('imgrid input imarray must be uint8')\n",
    "    pad = int(pad)\n",
    "    assert pad >= 0\n",
    "    cols = int(cols)\n",
    "    assert cols >= 1\n",
    "    N, H, W, C = imarray.shape\n",
    "    rows = int(np.ceil(N / float(cols)))\n",
    "    batch_pad = rows * cols - N\n",
    "    assert batch_pad >= 0\n",
    "    post_pad = [batch_pad, pad, pad, 0]\n",
    "    pad_arg = [[0, p] for p in post_pad]\n",
    "    imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n",
    "    H += pad\n",
    "    W += pad\n",
    "    grid = (imarray\n",
    "            .reshape(rows, cols, H, W, C)\n",
    "            .transpose(0, 2, 1, 3, 4)\n",
    "            .reshape(rows*H, cols*W, C))\n",
    "    if pad:\n",
    "        grid = grid[:-pad, :-pad]\n",
    "    return grid\n",
    "\n",
    "def imshow(a, format='png', jpeg_fallback=True, filename=None):\n",
    "    a = np.asarray(a, dtype=np.uint8)\n",
    "    str_file = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(str_file, format)\n",
    "    img = PIL.Image.fromarray(a)\n",
    "    im_data = str_file.getvalue()\n",
    "    try:\n",
    "        disp = IPython.display.display(IPython.display.Image(im_data))\n",
    "        if filename:\n",
    "            size = (a.shape[1]//2, a.shape[0]//2)\n",
    "            im = PIL.Image.fromarray(a)\n",
    "            im.thumbnail(size,PIL.Image.ANTIALIAS)\n",
    "            im.save('{}.{}'.format(filename, format))\n",
    "    except IOError:\n",
    "        if jpeg_fallback and format != 'jpeg':\n",
    "            print ('Warning: image was too large to display in '\n",
    "                   'format \"{}\"; trying jpeg instead.'.format(format))\n",
    "            return imshow(a, format='jpeg')\n",
    "        else:\n",
    "            raise\n",
    "    return disp, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "num_samples=6\n",
    "batch_size =1\n",
    "alpha_max = 100\n",
    "a = np.arange(-alpha_max,alpha_max+20, 20)\n",
    "\n",
    "random_seed = 6\n",
    "rnd = np.random.RandomState(random_seed)\n",
    "zs = rnd.randn(num_samples, dim_z)\n",
    "\n",
    "for batch_num, batch_start in enumerate(range(0, num_samples, batch_size)):\n",
    "    \n",
    "    ims = []\n",
    "    targets = []\n",
    "    \n",
    "    s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "\n",
    "    input_test = {z:zs[s]}\n",
    "\n",
    "    out_input_test = style_sess.run(outputs_orig, input_test)\n",
    "\n",
    "    for i in range(a.shape[0]):\n",
    "        # print(i)\n",
    "        alpha_val = a[i]\n",
    "        target_fn,_ = get_target_np(out_input_test, alpha_val) #, show_img=True, show_mask=True)\n",
    "        best_inputs = {z: zs[s], alpha2: alpha_val/alpha_max}\n",
    "#         best_inputs = {z: zs[s], alpha2: alpha_val/10} \n",
    "        best_im_out =  style_sess.run(transformed_output, best_inputs)\n",
    "        \n",
    "        # rescale\n",
    "        best_im_out = style_sess.run(uint8_im, {float_im: best_im_out})\n",
    "        target_fn = style_sess.run(uint8_im, {float_im: target_fn})\n",
    "        \n",
    "        ims.append(best_im_out)\n",
    "        targets.append(target_fn)\n",
    "    im_stack = np.concatenate(targets + ims).astype(np.uint8)\n",
    "    imshow(imgrid(im_stack, cols = len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss_vals_x = np.arange(400, 40400, 400)\n",
    "plt.plot(loss_vals_x, loss_vals)\n",
    "plt.xlabel('num samples, lr{}'.format(lr))\n",
    "plt.ylabel('Lpips')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can make a video\n",
    "import cv2\n",
    "ims_slerp = np.asarray(ims)\n",
    "img_show_size = 256\n",
    "fps = 10\n",
    "video_name = 'stylegan2_horse_shiftx.mp4'\n",
    "\n",
    "height = width = 256\n",
    "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MP4V'), fps=fps, frameSize=(width,height))\n",
    "for iter in range(0,ims_slerp.shape[0]):\n",
    "    fimg = np.squeeze(ims_slerp[iter,:,:,:])\n",
    "    fimg = cv2.resize(fimg, (256, 256))\n",
    "    a,b = imshow(fimg)\n",
    "    video.write(cv2.cvtColor(np.array(b), cv2.COLOR_BGR2RGB))\n",
    "video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenvpy36tf] *",
   "language": "python",
   "name": "conda-env-.conda-myenvpy36tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
